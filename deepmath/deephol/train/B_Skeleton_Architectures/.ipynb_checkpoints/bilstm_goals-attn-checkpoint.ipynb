{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 6., 9., ..., 0., 0., 0.],\n",
       "       [2., 6., 6., ..., 0., 0., 0.],\n",
       "       [2., 6., 6., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [2., 6., 6., ..., 0., 0., 0.],\n",
       "       [2., 6., 6., ..., 0., 0., 0.],\n",
       "       [2., 6., 9., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load numpy array from csv file\n",
    "from numpy import loadtxt\n",
    "# load array\n",
    "X_train = loadtxt('x_train2.csv', delimiter=',')\n",
    "Y_train = loadtxt('y_train2.csv', delimiter=',')\n",
    "# print the array\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  6.,  9., ...,  4.,  7.,  4.],\n",
       "       [ 2.,  6.,  6., ..., 10., 10., 21.],\n",
       "       [ 2.,  6.,  6., ...,  4.,  5.,  4.],\n",
       "       ...,\n",
       "       [ 2.,  6.,  6., ...,  0.,  0.,  0.],\n",
       "       [ 2.,  6.,  6., ...,  0.,  0.,  0.],\n",
       "       [ 2.,  6.,  9., ..., 15.,  7.,  7.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reducing the size of the input length so it can train on a CPU\n",
    "X_train = X_train[:, :100]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1254\n",
    "INPUT_LENGTH = 100 #1000\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dot product function\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a way to return attention weight vector a\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        # initialization of all learnable params\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        # regularizers for params, init as None\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        # constraints for params, init as None\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#         assert len(input_shape) == 3\n",
    "        \n",
    "        # weight matrix\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        # bias term\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        # context vector\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "#         a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def build_model(vocab_size, embedding_dim, input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=input_length))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Dense(41, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 128)          160512    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 256)          263168    \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 256)               66048     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 41)                10537     \n",
      "=================================================================\n",
      "Total params: 500,265\n",
      "Trainable params: 500,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, INPUT_LENGTH)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1800/1800 [==============================] - 26s 14ms/step - loss: 3.1554 - acc: 0.1339 - val_loss: 2.7767 - val_acc: 0.1800\n",
      "Epoch 2/5\n",
      "1800/1800 [==============================] - 19s 11ms/step - loss: 2.7415 - acc: 0.1411 - val_loss: 2.7840 - val_acc: 0.1800\n",
      "Epoch 3/5\n",
      "1800/1800 [==============================] - 19s 10ms/step - loss: 2.7176 - acc: 0.1511 - val_loss: 2.7533 - val_acc: 0.1750\n",
      "Epoch 4/5\n",
      "1800/1800 [==============================] - 19s 10ms/step - loss: 2.6928 - acc: 0.1661 - val_loss: 2.7433 - val_acc: 0.1700\n",
      "Epoch 5/5\n",
      "1800/1800 [==============================] - 19s 11ms/step - loss: 2.6872 - acc: 0.1711 - val_loss: 2.7188 - val_acc: 0.1450\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[1.96912885e-02 3.30536067e-03 1.18446974e-02 1.24880604e-01\n",
      " 1.38624683e-02 1.84409216e-01 1.29668726e-04 2.10610358e-03\n",
      " 9.58471298e-02 2.45568197e-04 5.69353532e-03 1.58555893e-04\n",
      " 2.15168251e-03 5.17400727e-02 1.26485684e-04 8.69503096e-02\n",
      " 3.24413739e-02 7.59687508e-04 2.68192409e-04 4.88667004e-03\n",
      " 1.33599853e-04 1.94943583e-04 4.46553677e-02 8.88536219e-04\n",
      " 7.46090189e-02 1.02181709e-03 2.04121266e-02 9.89629701e-03\n",
      " 3.77784148e-02 4.96099005e-03 1.45957267e-04 1.75513625e-02\n",
      " 3.75196673e-02 3.13366181e-03 6.33273320e-03 8.03843606e-04\n",
      " 3.80044710e-03 7.79126063e-02 8.79563019e-03 6.35619694e-03\n",
      " 1.59806700e-03]\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "example_x = X_train[0]\n",
    "print(np.shape(example_x))\n",
    "temp = model.predict(X_train[0:100])\n",
    "# print(len(temp)), temp\n",
    "print(temp[0])\n",
    "for i in temp:\n",
    "    print(np.argmax(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
