{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "import utils\n",
    "import data\n",
    "import extractor\n",
    "\n",
    "TRAIN = tf.estimator.ModeKeys.TRAIN\n",
    "EVAL = tf.estimator.ModeKeys.EVAL\n",
    "# PREDICT = tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "SOURCE_DATASETDIR = 0\n",
    "SOURCE_LOOPDIR = 1\n",
    "\n",
    "WAIT_SECONDS = 60\n",
    "# add test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 configuration\n",
    "\n",
    "config = {\n",
    "#     'AWS_ACCESS_KEY_ID':'AKIAR66VYUC6IKHLEWOV',            # Credentials only needed if connecting to a private endpoint\n",
    "#     'AWS_SECRET_ACCESS_KEY':'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',\n",
    "    'AWS_REGION':'us-east-2',                    # Region for the S3 bucket, this is not always needed. Default is us-east-1.\n",
    "    'S3_ENDPOINT':'s3.us-east-2.amazonaws.com',  # The S3 API Endpoint to connect to. This is specified in a HOST:PORT format.\n",
    "    'S3_USE_HTTPS':'1',                        # Whether or not to use HTTPS. Disable with 0.\n",
    "    'S3_VERIFY_SSL':'1',  \n",
    "}\n",
    "\n",
    "os.environ.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket='sagemaker-cs281'\n",
    "data_key = 'deephol-data/deepmath/deephol/proofs/human'\n",
    "\n",
    "ddir = 's3://{}/{}'.format(bucket, data_key)\n",
    "evalddir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataInfo(object):\n",
    "\n",
    "    def __init__(self,dataset_dir,eval_dataset_dir):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.eval_dataset_dir = eval_dataset_dir\n",
    "        self.ratio_neg_examples=7\n",
    "        self.ratio_max_hard_negative_examples=5\n",
    "        self.batch_size = 4\n",
    "        \n",
    "    def generate(self):\n",
    "        return {'dataset_dir': self.dataset_dir, 'eval_dataset_dir': self.eval_dataset_dir, 'ratio_neg_examples': \n",
    "                self.ratio_neg_examples, 'ratio_max_hard_negative_examples': self.ratio_max_hard_negative_examples, \n",
    "                'batch_size': self.batch_size,\n",
    "               } \n",
    "\n",
    "d = DataInfo(ddir,evalddir)\n",
    "hparams = d.generate()\n",
    "\n",
    "params = utils.Params(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_dir': 's3://sagemaker-cs281/deephol-data/deepmath/deephol/proofs/human',\n",
       " 'eval_dataset_dir': None,\n",
       " 'ratio_neg_examples': 7,\n",
       " 'ratio_max_hard_negative_examples': 5,\n",
       " 'batch_size': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ({goal: (), thms: (), thms_hard_negatives: (?,)}, {tac_id: ()}), types: ({goal: tf.string, thms: tf.string, thms_hard_negatives: tf.string}, {tac_id: tf.int64})>\n"
     ]
    }
   ],
   "source": [
    "train_data = data.get_holparam_dataset(TRAIN, params)\n",
    "eval_data = data.get_holparam_dataset(EVAL, params)\n",
    "\n",
    "# need to implement tristan_parser\n",
    "train_parsed = train_data.map(functools.partial(data.pairwise_thm_parser, params=params))\n",
    "print(train_parsed)\n",
    "\n",
    "# test for checking what train_parsed contains\n",
    "# for raw_record in train_parsed.take(10):\n",
    "#   print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:PASSED IN parser is None\n"
     ]
    }
   ],
   "source": [
    "input_fn = data.get_input_fn(dataset_fn=data.get_train_dataset, mode=TRAIN, params=params,\n",
    "                             shuffle_queue=10000,\n",
    "                             repeat=False)\n",
    "features, labels = input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
