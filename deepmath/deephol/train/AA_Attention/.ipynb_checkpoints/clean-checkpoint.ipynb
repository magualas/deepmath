{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Activation\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "import utilis\n",
    "\n",
    "# custom\n",
    "from keras.constraints import MinMaxNorm\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom python scripts\n",
    "from packages import generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'packages.generator' from '../packages/generator.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check version\n",
    "# print(inspect.getsource(generator.Keras_DataGenerator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM with Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "You are runnning an instance with 4 GPU's\n"
     ]
    }
   ],
   "source": [
    "# Check that you are running GPU's\n",
    "utilis.GPU_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS SETUP SHOULD BE COMPLETE, we are on <botocore.client.S3 object at 0x7f3d483fd940>\n"
     ]
    }
   ],
   "source": [
    "utilis.aws_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config, generators and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "INPUT_TENSOR_NAME = \"inputs_input\"\n",
    "SIGNATURE_NAME = \"serving_default\"\n",
    "W_HYP = False\n",
    "# LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# constnats\n",
    "VOCAB_SIZE = 1254\n",
    "INPUT_LENGTH = 3000 if W_HYP else 1000\n",
    "EMBEDDING_DIM = 512\n",
    "\n",
    "print(INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches:  5888.0\n",
      "# of batches reduced to:  235.52\n",
      "Generating examples from a set of 15073.28 examples \n",
      "\n",
      "\n",
      "# of batches:  1600.0\n",
      "# of batches reduced to:  64.0\n",
      "Generating examples from a set of 4096.0 examples \n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(generator)\n",
    "\n",
    "# generators\n",
    "training_generator = generator.Keras_DataGenerator(data_dir='', subset_frac = 0.04,  dataset='train_new', w_hyp=W_HYP)\n",
    "print()\n",
    "validation_generator = generator.Keras_DataGenerator(data_dir='', subset_frac = 0.04,  dataset='valid_new', w_hyp=W_HYP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dot product function\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "# find a way to return attention weight vector a\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        # initialization of all learnable params\n",
    "        self.init = initializers.get('lecun_uniform')\n",
    "        \n",
    "        # regularizers for params, init as None\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        # constraints for params, init as None\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#         assert len(input_shape) == 3\n",
    "        \n",
    "        # weight matrix\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        # bias term\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='lecun_uniform',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        # context vector\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "#         a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon()* 100, K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]  \n",
    "\n",
    "    \n",
    "# model\n",
    "def build_model(vocab_size, embedding_dim, input_length):\n",
    "    sequence_input = Input(shape=(input_length,), dtype='int32')\n",
    "    embedded_sequences = Embedding(vocab_size, embedding_dim, input_length=input_length)(sequence_input)\n",
    "    output_1 = SpatialDropout1D(0.9)(embedded_sequences)\n",
    "    output1 = Dropout(0.9)(output_1)\n",
    "    output_2 = Bidirectional(CuDNNLSTM(512, return_sequences=True, \n",
    "                                       kernel_constraint=MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), \n",
    "                                       recurrent_constraint=MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), \n",
    "                                       bias_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0)))(output_1)\n",
    "    context_vec = AttentionWithContext(\n",
    "        W_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0),\n",
    "        u_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0),\n",
    "        b_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0))(output_2)\n",
    "    \n",
    "    context_vec = Dropout(0.9)(context_vec)\n",
    "    predictions = Dense(41, kernel_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), activation='softmax')(context_vec)\n",
    "    model = Model(inputs=sequence_input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 1000, 512)         642048    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_9 (Spatial (None, 1000, 512)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 1000, 1024)        4202496   \n",
      "_________________________________________________________________\n",
      "attention_with_context_6 (At (None, 1024)              1050624   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 41)                42025     \n",
      "=================================================================\n",
      "Total params: 5,937,193\n",
      "Trainable params: 5,937,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, INPUT_LENGTH)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARE YOU LOADING A MODEL IF YES RUN TEH FOLLOWING LINES \n",
    "# from keras.models import model_from_json\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "# # REMEMEBER TO COMPILE \n",
    "# loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwriting model\n",
    "# model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[4].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/22\n",
      "235/235 [==============================] - 234s 998ms/step - loss: 2.8469 - acc: 0.1542 - val_loss: 2.7056 - val_acc: 0.1904\n",
      "Epoch 2/22\n",
      "235/235 [==============================] - 333s 1s/step - loss: 2.7212 - acc: 0.1751 - val_loss: 2.5501 - val_acc: 0.1968\n",
      "Epoch 3/22\n",
      "235/235 [==============================] - 229s 976ms/step - loss: 2.5725 - acc: 0.2205 - val_loss: 2.5090 - val_acc: 0.2021\n",
      "Epoch 4/22\n",
      "235/235 [==============================] - 229s 973ms/step - loss: 2.4730 - acc: 0.2459 - val_loss: 2.3773 - val_acc: 0.2703\n",
      "Epoch 5/22\n",
      "235/235 [==============================] - 224s 953ms/step - loss: 2.3971 - acc: 0.2696 - val_loss: 2.2765 - val_acc: 0.2930\n",
      "Epoch 6/22\n",
      "235/235 [==============================] - 226s 962ms/step - loss: 2.2934 - acc: 0.2878 - val_loss: 2.1951 - val_acc: 0.3088\n",
      "Epoch 7/22\n",
      "235/235 [==============================] - 227s 966ms/step - loss: 2.2386 - acc: 0.2894 - val_loss: 2.1775 - val_acc: 0.2988\n",
      "Epoch 8/22\n",
      "235/235 [==============================] - 225s 957ms/step - loss: 2.2692 - acc: 0.2853 - val_loss: 2.2039 - val_acc: 0.2859\n",
      "Epoch 9/22\n",
      "235/235 [==============================] - 223s 950ms/step - loss: 2.2480 - acc: 0.2860 - val_loss: 2.1750 - val_acc: 0.2864\n",
      "Epoch 10/22\n",
      "235/235 [==============================] - 224s 953ms/step - loss: 2.2168 - acc: 0.2903 - val_loss: 2.1624 - val_acc: 0.2922\n",
      "Epoch 11/22\n",
      "235/235 [==============================] - 224s 955ms/step - loss: 2.1952 - acc: 0.2935 - val_loss: 2.1519 - val_acc: 0.3040\n",
      "Epoch 12/22\n",
      "235/235 [==============================] - 224s 954ms/step - loss: 2.1686 - acc: 0.2986 - val_loss: 2.1186 - val_acc: 0.3171\n",
      "Epoch 13/22\n",
      "235/235 [==============================] - 225s 959ms/step - loss: 2.1382 - acc: 0.3029 - val_loss: 2.1014 - val_acc: 0.3083\n",
      "Epoch 14/22\n",
      "235/235 [==============================] - 223s 949ms/step - loss: 2.1145 - acc: 0.3091 - val_loss: 2.1133 - val_acc: 0.3118\n",
      "Epoch 15/22\n",
      "235/235 [==============================] - 224s 952ms/step - loss: 2.0840 - acc: 0.3195 - val_loss: 2.0949 - val_acc: 0.3105\n",
      "Epoch 16/22\n",
      "235/235 [==============================] - 226s 960ms/step - loss: 2.0798 - acc: 0.3225 - val_loss: 2.0737 - val_acc: 0.3247\n",
      "Epoch 17/22\n",
      "235/235 [==============================] - 222s 947ms/step - loss: 2.0658 - acc: 0.3258 - val_loss: 2.0563 - val_acc: 0.3191\n",
      "Epoch 18/22\n",
      "235/235 [==============================] - 222s 946ms/step - loss: 2.0585 - acc: 0.3280 - val_loss: 2.0825 - val_acc: 0.3223\n",
      "Epoch 19/22\n",
      "235/235 [==============================] - 224s 954ms/step - loss: 2.0574 - acc: 0.3289 - val_loss: 2.0584 - val_acc: 0.3176\n",
      "Epoch 20/22\n",
      "235/235 [==============================] - 226s 963ms/step - loss: 2.0466 - acc: 0.3309 - val_loss: 2.0449 - val_acc: 0.3435\n",
      "Epoch 21/22\n",
      "160/235 [===================>..........] - ETA: 59s - loss: 2.0624 - acc: 0.3236"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#try and make it run until 9 am GMT+1\n",
    "n_epochs = 22\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                            validation_data=validation_generator,\n",
    "                            verbose=1,\n",
    "                            use_multiprocessing=False,\n",
    "                            epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save modek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# FOR SAVING MODEL\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING_DECIDE_HOW_TO_NAME_LOG\n",
    "#descriptionofmodel_personwhostartsrun\n",
    "#e.g. LSTM_128encoder_etc_tanc\n",
    "LOSS_FILE_NAME = \"recs_1\"\n",
    "\n",
    "#WARNING NUMBER 2 - CURRENTLY EVERYTIME YOU RERUN THE CELLS BELOW THE FILES WITH THOSE NAMES GET WRITTEN OVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SOME LOGS -- OVERWROTE OLD LOGS -- SOMEONE NEEDS TO FIX THIS\n"
     ]
    }
   ],
   "source": [
    "# save history - WARNING FILE NAME \n",
    "utilis.history_saver_bad(history, LOSS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
