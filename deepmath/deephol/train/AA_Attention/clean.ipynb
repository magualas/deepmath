{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Activation\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "import utilis\n",
    "\n",
    "# custom\n",
    "from keras.constraints import MinMaxNorm\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom python scripts\n",
    "from packages import generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'packages.generator' from '../packages/generator.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check version\n",
    "# print(inspect.getsource(generator.Keras_DataGenerator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM with Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "You are runnning an instance with 4 GPU's\n"
     ]
    }
   ],
   "source": [
    "# Check that you are running GPU's\n",
    "utilis.GPU_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS SETUP SHOULD BE COMPLETE, we are on <botocore.client.S3 object at 0x7f3d483fd940>\n"
     ]
    }
   ],
   "source": [
    "utilis.aws_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config, generators and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "INPUT_TENSOR_NAME = \"inputs_input\"\n",
    "SIGNATURE_NAME = \"serving_default\"\n",
    "W_HYP = False\n",
    "# LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# constnats\n",
    "VOCAB_SIZE = 1254\n",
    "INPUT_LENGTH = 3000 if W_HYP else 1000\n",
    "EMBEDDING_DIM = 512\n",
    "\n",
    "print(INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches:  5888.0\n",
      "# of batches reduced to:  235.52\n",
      "Generating examples from a set of 15073.28 examples \n",
      "\n",
      "\n",
      "# of batches:  1600.0\n",
      "# of batches reduced to:  64.0\n",
      "Generating examples from a set of 4096.0 examples \n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(generator)\n",
    "\n",
    "# generators\n",
    "training_generator = generator.Keras_DataGenerator(data_dir='', subset_frac = 0.04,  dataset='train_new', w_hyp=W_HYP)\n",
    "print()\n",
    "validation_generator = generator.Keras_DataGenerator(data_dir='', subset_frac = 0.04,  dataset='valid_new', w_hyp=W_HYP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dot product function\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "# find a way to return attention weight vector a\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        # initialization of all learnable params\n",
    "        self.init = initializers.get('lecun_uniform')\n",
    "        \n",
    "        # regularizers for params, init as None\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        # constraints for params, init as None\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#         assert len(input_shape) == 3\n",
    "        \n",
    "        # weight matrix\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        # bias term\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='lecun_uniform',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        # context vector\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "#         a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon()* 100, K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]  \n",
    "\n",
    "    \n",
    "# model\n",
    "def build_model(vocab_size, embedding_dim, input_length):\n",
    "    sequence_input = Input(shape=(input_length,), dtype='int32')\n",
    "    embedded_sequences = Embedding(vocab_size, embedding_dim, input_length=input_length)(sequence_input)\n",
    "    output_1 = SpatialDropout1D(0.9)(embedded_sequences)\n",
    "    output1 = Dropout(0.9)(output_1)\n",
    "    output_2 = Bidirectional(CuDNNLSTM(512, return_sequences=True, \n",
    "                                       kernel_constraint=MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), \n",
    "                                       recurrent_constraint=MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), \n",
    "                                       bias_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0)))(output_1)\n",
    "    context_vec = AttentionWithContext(\n",
    "        W_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0),\n",
    "        u_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0),\n",
    "        b_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0))(output_2)\n",
    "    \n",
    "    context_vec = Dropout(0.9)(context_vec)\n",
    "    predictions = Dense(41, kernel_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), activation='softmax')(context_vec)\n",
    "    model = Model(inputs=sequence_input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 1000, 512)         642048    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_9 (Spatial (None, 1000, 512)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 1000, 1024)        4202496   \n",
      "_________________________________________________________________\n",
      "attention_with_context_6 (At (None, 1024)              1050624   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 41)                42025     \n",
      "=================================================================\n",
      "Total params: 5,937,193\n",
      "Trainable params: 5,937,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, INPUT_LENGTH)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARE YOU LOADING A MODEL IF YES RUN TEH FOLLOWING LINES \n",
    "# from keras.models import model_from_json\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "# # REMEMEBER TO COMPILE \n",
    "# loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwriting model\n",
    "# model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[4].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/22\n",
      "235/235 [==============================] - 234s 998ms/step - loss: 2.8469 - acc: 0.1542 - val_loss: 2.7056 - val_acc: 0.1904\n",
      "Epoch 2/22\n",
      "235/235 [==============================] - 333s 1s/step - loss: 2.7212 - acc: 0.1751 - val_loss: 2.5501 - val_acc: 0.1968\n",
      "Epoch 3/22\n",
      "235/235 [==============================] - 229s 976ms/step - loss: 2.5725 - acc: 0.2205 - val_loss: 2.5090 - val_acc: 0.2021\n",
      "Epoch 4/22\n",
      "235/235 [==============================] - 229s 973ms/step - loss: 2.4730 - acc: 0.2459 - val_loss: 2.3773 - val_acc: 0.2703\n",
      "Epoch 5/22\n",
      "235/235 [==============================] - 224s 953ms/step - loss: 2.3971 - acc: 0.2696 - val_loss: 2.2765 - val_acc: 0.2930\n",
      "Epoch 6/22\n",
      "235/235 [==============================] - 226s 962ms/step - loss: 2.2934 - acc: 0.2878 - val_loss: 2.1951 - val_acc: 0.3088\n",
      "Epoch 7/22\n",
      "235/235 [==============================] - 227s 966ms/step - loss: 2.2386 - acc: 0.2894 - val_loss: 2.1775 - val_acc: 0.2988\n",
      "Epoch 8/22\n",
      "235/235 [==============================] - 225s 957ms/step - loss: 2.2692 - acc: 0.2853 - val_loss: 2.2039 - val_acc: 0.2859\n",
      "Epoch 9/22\n",
      "235/235 [==============================] - 223s 950ms/step - loss: 2.2480 - acc: 0.2860 - val_loss: 2.1750 - val_acc: 0.2864\n",
      "Epoch 10/22\n",
      "235/235 [==============================] - 224s 953ms/step - loss: 2.2168 - acc: 0.2903 - val_loss: 2.1624 - val_acc: 0.2922\n",
      "Epoch 11/22\n",
      "235/235 [==============================] - 224s 955ms/step - loss: 2.1952 - acc: 0.2935 - val_loss: 2.1519 - val_acc: 0.3040\n",
      "Epoch 12/22\n",
      "235/235 [==============================] - 224s 954ms/step - loss: 2.1686 - acc: 0.2986 - val_loss: 2.1186 - val_acc: 0.3171\n",
      "Epoch 13/22\n",
      "235/235 [==============================] - 225s 959ms/step - loss: 2.1382 - acc: 0.3029 - val_loss: 2.1014 - val_acc: 0.3083\n",
      "Epoch 14/22\n",
      "235/235 [==============================] - 223s 949ms/step - loss: 2.1145 - acc: 0.3091 - val_loss: 2.1133 - val_acc: 0.3118\n",
      "Epoch 15/22\n",
      "235/235 [==============================] - 224s 952ms/step - loss: 2.0840 - acc: 0.3195 - val_loss: 2.0949 - val_acc: 0.3105\n",
      "Epoch 16/22\n",
      "235/235 [==============================] - 226s 960ms/step - loss: 2.0798 - acc: 0.3225 - val_loss: 2.0737 - val_acc: 0.3247\n",
      "Epoch 17/22\n",
      "235/235 [==============================] - 222s 947ms/step - loss: 2.0658 - acc: 0.3258 - val_loss: 2.0563 - val_acc: 0.3191\n",
      "Epoch 18/22\n",
      "235/235 [==============================] - 222s 946ms/step - loss: 2.0585 - acc: 0.3280 - val_loss: 2.0825 - val_acc: 0.3223\n",
      "Epoch 19/22\n",
      "235/235 [==============================] - 224s 954ms/step - loss: 2.0574 - acc: 0.3289 - val_loss: 2.0584 - val_acc: 0.3176\n",
      "Epoch 20/22\n",
      "235/235 [==============================] - 226s 963ms/step - loss: 2.0466 - acc: 0.3309 - val_loss: 2.0449 - val_acc: 0.3435\n",
      "Epoch 21/22\n",
      "215/235 [==========================>...] - ETA: 16s - loss: 2.0556 - acc: 0.3253"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#try and make it run until 9 am GMT+1\n",
    "n_epochs = 22\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                            validation_data=validation_generator,\n",
    "                            verbose=1,\n",
    "                            use_multiprocessing=False,\n",
    "                            epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save modek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# FOR SAVING MODEL\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING_DECIDE_HOW_TO_NAME_LOG\n",
    "#descriptionofmodel_personwhostartsrun\n",
    "#e.g. LSTM_128encoder_etc_tanc\n",
    "LOSS_FILE_NAME = \"recs_1\"\n",
    "\n",
    "#WARNING NUMBER 2 - CURRENTLY EVERYTIME YOU RERUN THE CELLS BELOW THE FILES WITH THOSE NAMES GET WRITTEN OVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SOME LOGS -- OVERWROTE OLD LOGS -- SOMEONE NEEDS TO FIX THIS\n"
     ]
    }
   ],
   "source": [
    "# save history - WARNING FILE NAME \n",
    "utilis.history_saver_bad(history, LOSS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches:  1920.0\n",
      "# of batches reduced to:  19.2\n",
      "Generating examples from a set of 1228.8 examples \n",
      "\n",
      "19/19 [==============================] - 17s 914ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0523844204450907, 0.31085526315789475]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator = generator.Keras_DataGenerator(data_dir='', subset_frac=0.01,  dataset='test_new', w_hyp=W_HYP)\n",
    "model.evaluate_generator(test_generator, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = np.mean(scores, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41,), (40,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medians.shape, np.array(lis).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Labels - Attention G')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(lis, medians)\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Labels - Attention G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis  = [x for x in range(1,42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
