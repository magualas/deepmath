{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesor Flow Version: 1.14.0 Ingest File\n",
      "Tesor Flow Version: 1.14.0  Utility File\n",
      "Tesor Flow Version: 1.14.0 Extactor 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "extraction_pipeline.py\n",
    "\n",
    "by Jeff, Manu and Tanc\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import boto3\n",
    "# print(tf.__version__)\n",
    "import ingestor\n",
    "import extractor2\n",
    "import data\n",
    "import functools\n",
    "import progressbar\n",
    "import inspect\n",
    "\n",
    "# config\n",
    "BUCKET_NAME = 'sagemaker-cs281'\n",
    "PARTITION_SIZE = 5000\n",
    "paths = {\n",
    "    'train':'deephol-data-processed/proofs/human/train',\n",
    "    'valid':'deephol-data-processed/proofs/human/valid',\n",
    "    'test':'deephol-data-processed/proofs/human/test'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split ='train'\n",
    "\n",
    "# # check\n",
    "# print(inspect.getsource(data.get_train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_np_to_s3(array, object_name):    \n",
    "    # save localy\n",
    "    local_filename = '/tmp/temp.csv'\n",
    "    np.savetxt(local_filename, array, delimiter=',')\n",
    "    \n",
    "    # s3 upload\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.upload_file(local_filename, BUCKET_NAME, object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/deepmath/deepmath/deephol/train/data.py:66: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/deepmath/deepmath/deephol/train/data.py:71: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/deepmath/deepmath/deephol/train/data.py:73: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/deepmath/deepmath/deephol/train/data.py:88: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- |                                     #        | 100710 Elapsed Time: 0:01:54"
     ]
    }
   ],
   "source": [
    "# make tf dataset of parsed examples\n",
    "params = ingestor.get_params()\n",
    "train_data = data.get_train_dataset(params)\n",
    "parser = data.tristan_parser\n",
    "train_parsed = train_data.map(functools.partial(parser, params=params))\n",
    "\n",
    "# set features and labels\n",
    "features = {'goal': [], 'goal_asl': [], 'thms': [], 'thms_hard_negatives': []}\n",
    "labels = {'tac_id': []}\n",
    "\n",
    "# iterate over dataset to extract data into arrays\n",
    "train_parsed = train_parsed # CHANGE HERE\n",
    "bar1 = progressbar.ProgressBar()\n",
    "for raw_record in bar1(train_parsed):\n",
    "    fx, lx = raw_record[0], raw_record[1]\n",
    "    features['goal'].append(fx['goal'])\n",
    "    features['goal_asl'].append(fx['goal_asl'])\n",
    "    features['thms'].append(fx['thms'])\n",
    "    features['thms_hard_negatives'].append(fx['thms_hard_negatives'])\n",
    "    labels['tac_id'].append(lx['tac_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate extractor object\n",
    "ex = extractor2.Extractor(params)\n",
    "\n",
    "# tokenize goals\n",
    "features['goal_ids'] = ex.tokenize(features['goal'], ex.vocab_table)\n",
    "\n",
    "# tokenize hypotheses\n",
    "length = len(features['goal'])\n",
    "features['goal_asl_ids'] = []\n",
    "for i in range(length):\n",
    "    temp = ex.tokenize(features['goal_asl'][i], ex.vocab_table)\n",
    "    features['goal_asl_ids'].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del features['goal']\n",
    "del features['goal_asl']\n",
    "del features['thms']\n",
    "del features['thms_hard_negatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['goal_ids'] is now an array of size N x 1000\n",
    "features['goal_ids'] = features['goal_ids'].numpy()\n",
    "print('Number of training examples:', len(features['goal_ids']))\n",
    "print('Size of training examples:', len(features['goal_ids'][0]))\n",
    "\n",
    "# features['goal_asl_ids'] is now an array of size  N x ? x 1000\n",
    "length = len(features['goal_asl_ids'])\n",
    "for i in range(length):\n",
    "    features['goal_asl_ids'][i] = [hypothesis.numpy() \n",
    "                                   for hypothesis in features['goal_asl_ids'][i]]\n",
    "print('Number of training examples:', len(features['goal_asl_ids']))\n",
    "print('Number of hypotheses for an example:', len(features['goal_asl_ids'][0]))\n",
    "print('Size of each hypothesis:', len(features['goal_asl_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['tactic_ids'] is now an array of size N x 1\n",
    "labels['tac_id'] = [i.numpy() for i in labels['tac_id']]\n",
    "print('Number of training examples:', len(labels['tac_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert goals to numpy arrays\n",
    "goals = np.array(features['goal_ids'])\n",
    "print(np.shape(goals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert goal hypotheses to numpy arrays and concatenate\n",
    "hypotheses = features['goal_asl_ids']\n",
    "length_hyp = len(hypotheses)\n",
    "for i in range(length_hyp):\n",
    "    if (len(hypotheses[i]) != 0):\n",
    "        hypotheses[i] = np.concatenate(hypotheses[i])  # concatenate hypotheses in a given hypothesis list\n",
    "        hypotheses[i] = hypotheses[i][hypotheses[i] != 0]  # remove zeroes in between\n",
    "        hypotheses[i] = hypotheses[i][0:3000]  # truncate to max hyp length = 3000 chars (< than 10% of data\n",
    "        len_conc = len(hypotheses[i]) # pad with zeroes to make length 3000 (to save as csv)\n",
    "        hypotheses[i] = np.pad(hypotheses[i], (0, 3000-len_conc), mode='constant')\n",
    "    else:\n",
    "        hypotheses[i] = np.zeros(3000, dtype = 'int32')\n",
    "np.set_printoptions(threshold=np.sys.maxsize)\n",
    "print(np.shape(hypotheses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tactics to numpy arrays and one-hot encode\n",
    "a = np.array(labels['tac_id'])\n",
    "tactics = np.zeros((a.size, 40+1))\n",
    "tactics[np.arange(a.size),a] = 1\n",
    "print(np.shape(tactics))\n",
    "\n",
    "X_train, Y_train = goals, tactics\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(hypotheses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature matrix with goals and hypotheses\n",
    "length = len(X_train)\n",
    "X_train_hyp = []\n",
    "for i in range(length):\n",
    "    train_example = np.concatenate((X_train[i], hypotheses[i]))  # concatenate goal and hypotheses\n",
    "    train_example = train_example[train_example != 0]  # remove zeroes in between\n",
    "    train_example = train_example[0:3000]  # truncate to max hyp length of 3000 chars (less than 10% of data\n",
    "    len_conc = len(train_example)  # pad with zeroes to make length 3000 (to save as csv)\n",
    "    train_example = np.pad(train_example, (0, 3000-len_conc), mode='constant')\n",
    "    X_train_hyp.append(np.asarray(train_example, dtype='float64').tolist())\n",
    "X_train_hyp = np.array(X_train_hyp)\n",
    "print(np.shape(X_train_hyp))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to s3\n",
    "partition_size = PARTITION_SIZE if len(Y_train) > PARTITION_SIZE else len(Y_train) \n",
    "n_partitions = len(Y_train) // partition_size\n",
    "print(len(Y_train), partition_size, n_partitions)\n",
    "for i, split in enumerate(np.array_split(X_train, n_partitions), 1):\n",
    "    upload_np_to_s3(split, os.path.join(paths[data_split], 'X_train_{}.csv'.format(i)))\n",
    "print('Uploaded all X_train files')\n",
    "for i, split in enumerate(np.array_split(X_train_hyp, n_partitions), 1):\n",
    "    upload_np_to_s3(split, os.path.join(paths[data_split], 'X_train_hyp_{}.csv'.format(i)))\n",
    "print('Uploaded all X_train_hyp files')\n",
    "upload_np_to_s3(Y_train, os.path.join(paths[data_split], 'Y_train.csv'))\n",
    "print('Uploaded Y_train file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
