{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# python packages pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Activation\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "import utilis\n",
    "\n",
    "# custom\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom python scripts\n",
    "from packages import generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'packages.generator' from '../packages/generator.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check version\n",
    "# print(inspect.getsource(generator.Keras_DataGenerator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM with Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "You are runnning an instance with 4 GPU's\n"
     ]
    }
   ],
   "source": [
    "# Check that you are running GPU's\n",
    "utilis.GPU_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS SETUP SHOULD BE COMPLETE, we are on <botocore.client.S3 object at 0x7fe9f4122358>\n"
     ]
    }
   ],
   "source": [
    "utilis.aws_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config, generators and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "INPUT_TENSOR_NAME = \"inputs_input\"\n",
    "SIGNATURE_NAME = \"serving_default\"\n",
    "W_HYP = True\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# constnats\n",
    "VOCAB_SIZE = 1254\n",
    "INPUT_LENGTH = 3000 if W_HYP else 1000\n",
    "EMBEDDING_DIM = 512\n",
    "\n",
    "print(INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches:  5888.0\n",
      "Generating examples from a set of 376832 examples \n",
      "\n",
      "\n",
      "# of batches:  1600.0\n",
      "Generating examples from a set of 102400 examples \n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(generator)\n",
    "\n",
    "# generators\n",
    "training_generator = generator.Keras_DataGenerator(data_dir='',  dataset='train_new', w_hyp=W_HYP)\n",
    "print()\n",
    "validation_generator = generator.Keras_DataGenerator(data_dir='', dataset='valid_new', w_hyp=W_HYP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dot product function\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "# find a way to return attention weight vector a\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        # initialization of all learnable params\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        # regularizers for params, init as None\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        # constraints for params, init as None\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#         assert len(input_shape) == 3\n",
    "        \n",
    "        # weight matrix\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        # bias term\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='lecun_uniform',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        # context vector\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "#         a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon() * 100, K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]  \n",
    "\n",
    "    \n",
    "# model\n",
    "def build_model(vocab_size, embedding_dim, input_length):\n",
    "    sequence_input = Input(shape=(input_length,), dtype='int32')\n",
    "    embedded_sequences = Embedding(vocab_size, embedding_dim, input_length=input_length)(sequence_input)\n",
    "    output_1 = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "    output_2 = Bidirectional(CuDNNLSTM(512, return_sequences=True))(output_1)\n",
    "    context_vec = AttentionWithContext()(output_2)\n",
    "    predictions = Dense(41, activation='softmax')(context_vec)\n",
    "    model = Model(inputs=sequence_input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 3000, 512)         642048    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_10 (Spatia (None, 3000, 512)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 3000, 1024)        4202496   \n",
      "_________________________________________________________________\n",
      "attention_with_context_10 (A (None, 1024)              1050624   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 41)                42025     \n",
      "=================================================================\n",
      "Total params: 5,937,193\n",
      "Trainable params: 5,937,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, INPUT_LENGTH)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARE YOU LOADING A MODEL IF YES RUN TEH FOLLOWING LINES \n",
    "# from keras.models import model_from_json\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "# # REMEMEBER TO COMPILE \n",
    "# loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwriting model\n",
    "# model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.1365041e-02, -3.7061837e-02, -8.3468631e-03, ...,\n",
       "          2.6247233e-05,  4.5909718e-02, -1.6048344e-02],\n",
       "        [-4.6823993e-02, -3.2539304e-02,  1.6552424e-02, ...,\n",
       "         -3.5134781e-02, -3.1592704e-02,  5.9929532e-03],\n",
       "        [ 1.8258411e-02,  1.5940977e-02, -2.7254185e-02, ...,\n",
       "          2.0595365e-03, -4.2403486e-02, -3.0106008e-02],\n",
       "        ...,\n",
       "        [ 3.1335138e-02,  2.5152473e-02,  2.1871361e-03, ...,\n",
       "         -4.0942155e-02, -3.0095126e-02,  3.1416127e-03],\n",
       "        [ 4.5849313e-03,  3.8136318e-02,  3.3987521e-03, ...,\n",
       "          4.6139356e-02,  3.7941877e-02,  2.9278230e-03],\n",
       "        [-3.8018864e-02,  2.6216814e-02, -2.6242482e-02, ...,\n",
       "          1.0189861e-02, -1.8183669e-02, -1.9201607e-02]], dtype=float32),\n",
       " array([[-0.02332686,  0.02809873, -0.01245729, ...,  0.02741083,\n",
       "         -0.01322309,  0.01913308],\n",
       "        [-0.01871773, -0.01041946, -0.03736302, ...,  0.0449085 ,\n",
       "         -0.02017113,  0.02510605],\n",
       "        [ 0.03911094,  0.02257312, -0.0097738 , ...,  0.00890985,\n",
       "         -0.00823569, -0.01952918],\n",
       "        ...,\n",
       "        [-0.0210594 ,  0.0014409 , -0.00794192, ...,  0.00103885,\n",
       "         -0.01581211, -0.03886689],\n",
       "        [-0.01223465, -0.00596553, -0.02021999, ...,  0.01037869,\n",
       "          0.01486619, -0.01390244],\n",
       "        [ 0.00415722, -0.02678052, -0.01942483, ..., -0.02379548,\n",
       "         -0.01509796,  0.01018445]], dtype=float32),\n",
       " array([4.1223204e-04, 4.3917174e-04, 1.3165653e-04, ..., 6.0226675e-04,\n",
       "        9.5667451e-04, 8.4761814e-05], dtype=float32),\n",
       " array([[ 0.00792077, -0.00813217, -0.0406802 , ...,  0.03329811,\n",
       "          0.01790746,  0.048191  ],\n",
       "        [-0.0068975 , -0.02868386,  0.01328624, ..., -0.04173833,\n",
       "         -0.04172347, -0.0060641 ],\n",
       "        [ 0.03341482,  0.0063156 ,  0.0322702 , ...,  0.00045285,\n",
       "         -0.02602902, -0.00587597],\n",
       "        ...,\n",
       "        [ 0.00154125, -0.00699418,  0.043057  , ...,  0.00055106,\n",
       "         -0.00350867,  0.01094683],\n",
       "        [ 0.01945672,  0.04775469, -0.02990244, ..., -0.03665186,\n",
       "          0.00828902,  0.01946158],\n",
       "        [ 0.01871987,  0.02937834,  0.00252465, ..., -0.00539779,\n",
       "          0.04418659, -0.00027897]], dtype=float32),\n",
       " array([[ 0.06009192, -0.01684514,  0.03619298, ...,  0.02001296,\n",
       "          0.02552139, -0.00817956],\n",
       "        [ 0.01085446, -0.0074942 ,  0.01973698, ..., -0.0164584 ,\n",
       "          0.00464339, -0.01994532],\n",
       "        [-0.00044853,  0.00157779,  0.01794599, ...,  0.0188948 ,\n",
       "         -0.00137386,  0.00992898],\n",
       "        ...,\n",
       "        [ 0.01810208, -0.03148421,  0.0090335 , ..., -0.01356571,\n",
       "         -0.03092678, -0.01424786],\n",
       "        [ 0.02104556, -0.00193316, -0.00614457, ..., -0.00092861,\n",
       "         -0.01289565,  0.03333909],\n",
       "        [ 0.00014959, -0.03867727, -0.02999785, ...,  0.01341583,\n",
       "         -0.0032712 ,  0.00866996]], dtype=float32),\n",
       " array([ 2.5045096e-03, -9.7918579e-05,  4.9995794e-04, ...,\n",
       "         6.5205595e-04,  4.4745061e-04,  4.0952975e-05], dtype=float32)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 2203s 2s/step - loss: 2.6808 - acc: 0.1935 - val_loss: 2.7159 - val_acc: 0.1846\n",
      "CPU times: user 14min 28s, sys: 4min 20s, total: 18min 48s\n",
      "Wall time: 36min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#try and make it run until 9 am GMT+1\n",
    "n_epochs = 1\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                            validation_data=validation_generator,\n",
    "                            verbose=1,\n",
    "                            use_multiprocessing=True,\n",
    "                            epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save modek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SAVING MODEL\n",
    "model_json = model_GPU.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING_DECIDE_HOW_TO_NAME_LOG\n",
    "#descriptionofmodel_personwhostartsrun\n",
    "#e.g. LSTM_128encoder_etc_tanc\n",
    "LOSS_FILE_NAME = \"forjeff1\"\n",
    "\n",
    "#WARNING NUMBER 2 - CURRENTLY EVERYTIME YOU RERUN THE CELLS BELOW THE FILES WITH THOSE NAMES GET WRITTEN OVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_logs/forjeff1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-1ff75c34f1cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save history - WARNING FILE NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mutilis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_saver_bad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOSS_FILE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/deepmath/deepmath/deephol/train/AA_Attention2/utilis.py\u001b[0m in \u001b[0;36mhistory_saver_bad\u001b[0;34m(history, LOSS_FILE_NAME)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#TODO: read file if it exists merge two files rather than overwriting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mnumpy_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_logs/\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mLOSS_FILE_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# save full history json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;31m# datasource doesn't support creating a new file ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0mown_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_logs/forjeff1.csv'"
     ]
    }
   ],
   "source": [
    "# save history - WARNING FILE NAME \n",
    "utilis.history_saver_bad(history, LOSS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
