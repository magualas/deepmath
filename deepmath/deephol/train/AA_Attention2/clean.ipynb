{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Activation\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "import utilis\n",
    "\n",
    "# custom\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.constraints import MinMaxNorm\n",
    "from keras import initializers, regularizers, constraints, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom python scripts\n",
    "from packages import generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'packages.generator' from '../packages/generator.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check version\n",
    "# print(inspect.getsource(generator.Keras_DataGenerator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM with Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "You are runnning an instance with 4 GPU's\n"
     ]
    }
   ],
   "source": [
    "# Check that you are running GPU's\n",
    "utilis.GPU_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS SETUP SHOULD BE COMPLETE, we are on <botocore.client.S3 object at 0x7f62b847a940>\n"
     ]
    }
   ],
   "source": [
    "utilis.aws_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config, generators and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "INPUT_TENSOR_NAME = \"inputs_input\"\n",
    "SIGNATURE_NAME = \"serving_default\"\n",
    "W_HYP = True\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# constnats\n",
    "VOCAB_SIZE = 1254\n",
    "INPUT_LENGTH = 3000 if W_HYP else 1000\n",
    "EMBEDDING_DIM = 512\n",
    "\n",
    "print(INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches:  5888.0\n",
      "# of batches reduced to:  235.52\n",
      "Generating examples from a set of 15073.28 examples \n",
      "\n",
      "\n",
      "# of batches:  1600.0\n",
      "# of batches reduced to:  64.0\n",
      "Generating examples from a set of 4096.0 examples \n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(generator)\n",
    "\n",
    "# generators\n",
    "training_generator = generator.Keras_DataGenerator(data_dir='', subset_frac=0.04,  dataset='train_new', w_hyp=W_HYP)\n",
    "print()\n",
    "validation_generator = generator.Keras_DataGenerator(data_dir='', subset_frac=0.04, dataset='valid_new', w_hyp=W_HYP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dot product function\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "# find a way to return attention weight vector a\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        # initialization of all learnable params\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        # regularizers for params, init as None\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        # constraints for params, init as None\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#         assert len(input_shape) == 3\n",
    "        \n",
    "        # weight matrix\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        # bias term\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='lecun_uniform',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        # context vector\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "#         a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon() * 100, K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]  \n",
    "\n",
    "    \n",
    "# model\n",
    "def build_model(vocab_size, embedding_dim, input_length):\n",
    "    sequence_input = Input(shape=(input_length,), dtype='int32')\n",
    "    embedded_sequences = Embedding(vocab_size, embedding_dim, input_length=input_length)(sequence_input)\n",
    "    output_1 = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "    output_2 = Bidirectional(CuDNNLSTM(512, return_sequences=True, \n",
    "                                       kernel_constraint=MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), \n",
    "                                       recurrent_constraint=MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), \n",
    "                                       bias_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0)))(output_1)\n",
    "    context_vec = AttentionWithContext(\n",
    "        W_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0),\n",
    "        u_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0),\n",
    "        b_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0))(output_2)\n",
    "    predictions = Dense(41, kernel_constraint = MinMaxNorm(min_value=0.0001, max_value=1.0, rate=1.0, axis=0), activation='softmax')(context_vec)\n",
    "    model = Model(inputs=sequence_input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 3000, 512)         642048    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_10 (Spatia (None, 3000, 512)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 3000, 1024)        4202496   \n",
      "_________________________________________________________________\n",
      "attention_with_context_10 (A (None, 1024)              1050624   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 41)                42025     \n",
      "=================================================================\n",
      "Total params: 5,937,193\n",
      "Trainable params: 5,937,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, INPUT_LENGTH)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARE YOU LOADING A MODEL IF YES RUN TEH FOLLOWING LINES \n",
    "# from keras.models import model_from_json\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "# # REMEMEBER TO COMPILE \n",
    "# loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwriting model\n",
    "# model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.0217997 ,  0.03969083,  0.0215626 , ...,  0.03818964,\n",
       "          0.01074576,  0.04020418],\n",
       "        [-0.00333207, -0.05536939, -0.05589694, ...,  0.03243203,\n",
       "          0.00509956, -0.04171598],\n",
       "        [ 0.0343883 ,  0.04647451, -0.00663181, ..., -0.00550194,\n",
       "         -0.00727929, -0.01725633],\n",
       "        ...,\n",
       "        [ 0.04436937, -0.02148931, -0.00088677, ..., -0.01644387,\n",
       "          0.0431393 , -0.01350097],\n",
       "        [ 0.02521509,  0.03857413,  0.021363  , ...,  0.03159701,\n",
       "         -0.00033784, -0.03753368],\n",
       "        [ 0.01121367,  0.00749657, -0.02630752, ...,  0.0053083 ,\n",
       "          0.02673394, -0.03542515]], dtype=float32),\n",
       " array([ 0.04667596, -0.02666715, -0.03496671, ...,  0.04108815,\n",
       "         0.03442511,  0.02041246], dtype=float32),\n",
       " array([-0.01997736, -0.02414181,  0.02603782, ...,  0.03218105,\n",
       "         0.0073827 , -0.03639194], dtype=float32)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[4].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "235/235 [==============================] - 635s 3s/step - loss: 2.6802 - acc: 0.1906 - val_loss: 2.6020 - val_acc: 0.2065\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 600s 3s/step - loss: 2.5519 - acc: 0.2275 - val_loss: 2.6120 - val_acc: 0.1992\n",
      "CPU times: user 5min 14s, sys: 1min 34s, total: 6min 49s\n",
      "Wall time: 20min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#try and make it run until 9 am GMT+1\n",
    "n_epochs = 2\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                            validation_data=validation_generator,\n",
    "                            verbose=1,\n",
    "                            use_multiprocessing=True,\n",
    "                            epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save modek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SAVING MODEL\n",
    "model_json = model_GPU.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING_DECIDE_HOW_TO_NAME_LOG\n",
    "#descriptionofmodel_personwhostartsrun\n",
    "#e.g. LSTM_128encoder_etc_tanc\n",
    "LOSS_FILE_NAME = \"forjeff2\"\n",
    "\n",
    "#WARNING NUMBER 2 - CURRENTLY EVERYTIME YOU RERUN THE CELLS BELOW THE FILES WITH THOSE NAMES GET WRITTEN OVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED SOME LOGS -- OVERWROTE OLD LOGS -- SOMEONE NEEDS TO FIX THIS\n"
     ]
    }
   ],
   "source": [
    "# save history - WARNING FILE NAME \n",
    "utilis.history_saver_bad(history, LOSS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
